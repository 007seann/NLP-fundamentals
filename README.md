# NLP-fundamentals

This repository contains my implementations of fundamental NLP architectures **from scratch**, built to strengthen intuition and practical skills.  
I focused on understanding the math, training loop, and building blocks, rather than relying on pre-built libraries.

## Topics Covered
1. **Transformer**  
   - Scaled dot-product attention, multi-head attention, positional encoding.  
   - Encoder–decoder architecture implemented from scratch.  
   - Trained on toy translation datasets.  

2. **GRU (Gated Recurrent Unit)**  
   - Step-by-step forward pass implementation.  
   - Comparison with vanilla RNN and LSTM.  
   - Applied on sequence classification task.  

3. **LSTM (Long Short-Term Memory)**  
   - Implemented with forget, input, and output gates.  
   - Demonstrated on language modeling (next-word prediction).  

4. **Machine Translation**  
   - Sequence-to-sequence (Seq2Seq) with attention.  
   - Compared RNN-based and Transformer-based translation.  
   - Hands-on experiments with English ↔ German dataset (toy version).  

## Skills Demonstrated
- Deep understanding of core NLP architectures.  
- From-scratch PyTorch implementations.  
- Handling sequence data, padding, batching.  
- Training & evaluation on real datasets.  

## Results
- GRU & LSTM models achieve good perplexity on small language modeling tasks.  
- Transformer-based translation shows clear performance improvements over Seq2Seq RNNs.  




